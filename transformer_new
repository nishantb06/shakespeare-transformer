import os
from dataclasses import dataclass
from typing import Optional, List, Dict, Any, Tuple

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.nn import Transformer
import torch.nn.functional as F


@dataclass
class Config:
    vocab_size: int = 50257
    n_heads: int = 12
    dim: int = 768
    max_seq_length: int = 2048
    num_layers: int = 12
    dropout: float = 0.1


class MultiHeadAttention(nn.Module):
    def __init__(self, config) -> None:
        super().__init__()
        self.n_dim = config.dim
        self.config = config
        self.n_heads = self.config.n_heads
        self.dropout = self.config.dropout

        self.qkv_weights = nn.Linear(self.n_dim, self.n_dim * 3)
        self.projection_layer = nn.Linear(self.n_dim, self.n_dim)

        self.attention_dropout = nn.Dropout(self.dropout)
        self.projection_dropout = nn.Dropout(self.dropout)

    # dimension of x to be (batch_size, token_length, dim) => (B, C, D)
    # return a tensor of similar shape but with attention scores weighted averaged value vectors
    def forward(self, x):
        B, T, C = x.size()
        q, k, v = self.qkv_weights(x).split(
            self.n_dim, dim=-1
        )  # each of q,k v have a shape of (B,C,D)
        q = q.view(B, T, self.n_heads, C // self.n_heads).transpose(
            1, 2
        )  # (B, N_HEADS , T , D // N_HEADS) => (B, 12, T , 64)
        k = k.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)
        v = v.view(B, T, self.n_heads, C // self.n_heads).transpose(1, 2)

        attention_matrix = (q @ k.transpose(-2, -1)) * (1 / q.size(-1) ** 0.5)
        attention_matrix = F.softmax(attention_matrix, dim=-1)  # (B, N_HEADS, T, T)
        attention_matrix = self.attention_dropout(
            attention_matrix
        )  # (B, N_HEADS, T, T)

        v = (
            (attention_matrix @ v)
            .transpose(2, 1)
            .contiguous()
            .view(
                B, T, C
            )  # the contiguous is important because , to make memory contiguous
        )  # torch.Size([1, 32, 768]) [B, T, C]
        v = self.projection_layer(v)
        v = self.projection_dropout(v)
        return v
    
class FeedForward(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.c_fc = nn.Linear(config.dim, 4 * config.dim) # [n_embd, 4 * n_embd]
        self.c_proj = nn.Linear(4 * config.dim, config.dim) # [4 * n_embd, n_embd]
        self.dropout = nn.Dropout(config.dropout)

    def forward(self, x):
        x = self.c_fc(x) # [B, T, 4 * n_embd]
        x = F.gelu(x) # [B, T, 4 * n_embd]
        x = self.c_proj(x) # [B, T, n_embd]
        x = self.dropout(x) # [B, T, n_embd]
        return x


class TransformerBlock(nn.Module):
    def __init__(self) -> None:
        pass

    def forward(self) -> None:
        pass


class DecoderOnlyTransformer(nn.Module):
    def __init__(self) -> None:
        super.__init__()

    def forward(self) -> None:
        pass
