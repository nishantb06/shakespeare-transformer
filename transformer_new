import os
from dataclasses import dataclass
from typing import Optional, List, Dict, Any, Tuple

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.nn import Transformer


@dataclass
class Config:
    vocab_size: int = 50257
    n_heads: int = 12
    dim: int = 768
    max_seq_length: int = 2048
    num_layers: int = 12
    dropout: float = 0.1


class MultiHeadAttention(nn.Module):
    def __init__(self, config) -> None:
        super().__init__()
        self.n_dim = config.dim
        self.config = config
        self.n_heads = self.config.n_heads
        self.qkv_weights = nn.Linear(self.n_dim, self.n_dim * 3)

    # dimension of x to be (batch_size, token_length, dim) => (B, C, D)
    # return a tensor of similar shape but with attention scores weighted averaged value vectors
    def forward(self, x):
        B, T, C = x.size()
        q, k, v = self.qkv_weights(x).split(
            self.n_dim, dim=-1
        )  # each of q,k v have a shape of (B,C,D)
        q = q.view(B, T, self.n_heads, C // self.n_heads).transpose(
            2, 1
        )  # (B, N_HEADS , T , D // N_HEADS) => (B, 12, T , 64)
        k = (
            k.view(B, T, self.n_heads, C // self.n_heads)
            .transpose(2, 1)
            .transpose(-2, -1)
        )
        v = v.view(B, T, self.n_heads, C // self.n_heads).transpose(2, 1)

        attention_matrix = (torch.matmul(q, k) / (q.shape[-1] ** 0.5)).softmax(
            dim=-1
        )  # torch.Size([1, 12, 32, 32]) [B, num_heads, T, T]
        v = (
            (attention_matrix @ v).transpose(2, 1).reshape(B, T, -1)
        )  # torch.Size([1, 32, 768]) [B, T, C]
        return v


class TransformerBlock(nn.Module):
    def __init__(self) -> None:
        pass

    def forward(self) -> None:
        pass


class DecoderOnlyTransformer(nn.Module):
    def __init__(self) -> None:
        super.__init__()

    def forward(self) -> None:
        pass
